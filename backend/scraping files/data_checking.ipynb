{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52c8924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technical support: 10\n",
      "support engineer: 10\n",
      "d3 data: 5\n",
      "data analyst: 5\n",
      "analyst production: 5\n",
      "production staff: 5\n",
      "staff code: 5\n",
      "code prod: 5\n",
      "prod da: 5\n",
      "d3 data analyst: 5\n",
      "data analyst production: 5\n",
      "analyst production staff: 5\n",
      "production staff code: 5\n",
      "staff code prod: 5\n",
      "code prod da: 5\n",
      "d3 data analyst production: 5\n",
      "data analyst production staff: 5\n",
      "analyst production staff code: 5\n",
      "production staff code prod: 5\n",
      "staff code prod da: 5\n",
      "business analyst: 5\n",
      "analyst level: 5\n",
      "business analyst level: 5\n",
      "helpdesk engineer: 5\n",
      "engineer technical: 5\n",
      "helpdesk engineer technical: 5\n",
      "engineer technical support: 5\n",
      "technical support engineer: 5\n",
      "helpdesk engineer technical support: 5\n",
      "engineer technical support engineer: 5\n",
      "operation edp: 5\n",
      "edp support: 5\n",
      "support staff: 5\n",
      "operation edp support: 5\n",
      "edp support staff: 5\n",
      "operation edp support staff: 5\n",
      "staff product: 5\n",
      "product strategy: 5\n",
      "staff product strategy: 5\n",
      "customer care: 5\n",
      "care experience: 5\n",
      "experience associate: 5\n",
      "customer care experience: 5\n",
      "care experience associate: 5\n",
      "customer care experience associate: 5\n",
      "customer solution: 5\n",
      "solution associate: 5\n",
      "associate months: 5\n",
      "months contract: 5\n",
      "customer solution associate: 5\n",
      "solution associate months: 5\n",
      "associate months contract: 5\n",
      "customer solution associate months: 5\n",
      "solution associate months contract: 5\n",
      "website administrator: 5\n",
      "staff programmer: 5\n",
      "developer staff: 5\n",
      "project admin: 5\n",
      "field supervisor: 5\n",
      "supervisor telecommunications: 5\n",
      "telecommunications infrastructure: 5\n",
      "infrastructure serang: 5\n",
      "field supervisor telecommunications: 5\n",
      "supervisor telecommunications infrastructure: 5\n",
      "telecommunications infrastructure serang: 5\n",
      "field supervisor telecommunications infrastructure: 5\n",
      "supervisor telecommunications infrastructure serang: 5\n",
      "erp sap: 5\n",
      "sap functional: 5\n",
      "functional staff: 5\n",
      "erp sap functional: 5\n",
      "sap functional staff: 5\n",
      "erp sap functional staff: 5\n",
      "server network: 5\n",
      "network engineer: 5\n",
      "server network engineer: 5\n",
      "solution specialist: 5\n",
      "production engineering: 5\n",
      "engineering programmer: 5\n",
      "production engineering programmer: 5\n",
      "staff programs: 5\n",
      "strategy analyst: 2\n",
      "quality assurance: 1\n",
      "assurance qa: 1\n",
      "qa engineer: 1\n",
      "quality assurance qa: 1\n",
      "assurance qa engineer: 1\n",
      "quality assurance qa engineer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\gcolab\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'bulanan', 'description', 'gaji', 'jam', 'job', 'kali', 'kurangnya', 'lembur', 'makan', 'mata', 'olah', 'online', 'pokok', 'raya', 'sekurang', 'setidak', 'tama', 'tanggung', 'tidaknya', 'today', 'transport', 'tunjangan', 'uang'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %pip install langdetect\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# %pip install deep-translator\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "df_jobs = pd.read_csv('jobstreet_techjobs_Jul2025.csv')\n",
    "\n",
    "def is_en_or_id(text):\n",
    "    try:\n",
    "        lang = detect(str(text))\n",
    "        return lang in ['en', 'id']\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df_jobs = df_jobs[df_jobs['job_details'].apply(is_en_or_id)].reset_index(drop=True)\n",
    "\n",
    "# stopwords umum + custom \n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('indonesian')))\n",
    "custom_stopwords = set([\n",
    "    'pt', 'cv', 'tbk', 'persero', 'dkk', 'dll', 'dst',\n",
    "    'required', 'requirement', 'requirements',\n",
    "    'qualifications', 'qualification', 'kualifikasi', 'job description',\n",
    "    'primary', 'main', ''\n",
    "    'responsibility', 'responsibilities',\n",
    "    'benefits', 'benefit', 'tanggung jawab',\n",
    "    'Gaji Pokok', 'gaji bulanan', 'gaji per bulan',\n",
    "    'gaji per tahun', 'gaji per jam', 'gaji per hari', \n",
    "    'uang makan', 'uang transport', 'uang lembur', 'bonus',\n",
    "    'tunjangan hari raya', 'thr', 'seeking', 'join', 'tugas', 'welcome',\n",
    "    'apply', 'apply now', 'apply here', 'apply today', 'apply online',\n",
    "    'utama'\n",
    "])\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "# normalisasi teks \n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    emoji_pattern = re.compile(\"[\"                     \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\" \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_jobs = df_jobs[df_jobs['job_details'].apply(is_en_or_id)].reset_index(drop=True)\n",
    "df_jobs['job_details'] = df_jobs['job_details'].apply(normalize_text)\n",
    "all_words = []\n",
    "for text in df_jobs['job_details']:\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in all_stopwords]\n",
    "    all_words.extend(filtered_tokens)\n",
    "\n",
    "\n",
    "titles = df_jobs['job_title'].astype(str).tolist()\n",
    "translated_titles = [GoogleTranslator(source='auto', target='en').translate(job_title) for job_title in titles]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(2, 4),\n",
    "    stop_words=list(all_stopwords),  \n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(translated_titles)\n",
    "sum_words = X.sum(axis=0)\n",
    "phrases_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "phrases_freq = sorted(phrases_freq, key=lambda x: x[1], reverse=True)[:90]\n",
    "\n",
    "for phrase, freq in phrases_freq:\n",
    "    print(f\"{phrase}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d2062d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'topic_tag':\n",
      "['Data' 'Full Stack, Front End' 'Computer Science, Softskill'\n",
      " 'Data, Programming Language' 'AI, Data' 'Full Stack'\n",
      " 'Machine Learning, Data' 'Programming Language' 'AI, Machine Learning'\n",
      " 'Full Stack, Cloud Computing' 'Android, Machine Learning' 'Softskill'\n",
      " 'Android' 'Full Stack, Back End' 'AI' 'Machine Learning, Flutter'\n",
      " 'Full Stack, DevOps' 'Computer Science, DevOps' 'Cloud Computing, DevOps'\n",
      " 'Cloud Computing' 'UI/UX' 'Flutter, Mobile Development'\n",
      " 'Android, Mobile Development' 'iOS, Android' 'Android, Flutter' 'iOS'\n",
      " 'Cloud Computing, AWS']\n",
      "\n",
      "Unique values in 'learning_path':\n",
      "['Data Scientist' nan 'Back-End  JavaScript' 'DevOps Engineer' 'React'\n",
      " 'Android' 'Google Cloud Professional' 'Multi-Platform App'\n",
      " 'Machine Learning Engineer' 'Machine Learning Engineer, Data Scientist'\n",
      " 'Android, iOS, Multi-Platform App' 'iOS' 'Front-End Web, React'\n",
      " 'React, Front-End Web' 'Front-End Web' 'Back-End, JavaScript'\n",
      " 'Google Cloud Professional, React, Front-End Web, Back-End, JavaScript']\n"
     ]
    }
   ],
   "source": [
    "df_course = pd.read_csv('dicoding_course_details.csv')\n",
    "\n",
    "df_course['topic_tags'] = df_course['topic_tags'].str.replace(r'\\d+', '', regex=True).str.replace(r',\\s*,', ',', regex=True).str.strip(', ')\n",
    "# Save the DataFrame after removing numbers from 'topic_tags'\n",
    "df_course.to_csv('dicoding_course_details.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Unique values in 'topic_tag':\")\n",
    "print(df_course['topic_tags'].unique())\n",
    "\n",
    "print(\"\\nUnique values in 'learning_path':\")\n",
    "print(df_course['learning_path'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2aaabc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original unique values in 'learning_path':\n",
      "['Data Scientist'\n",
      " 'Data Scientist nan Back-End JavaScript DevOps Engineer React'\n",
      " 'Android \\n' ', , iOS \\n' 'Front-End Web \\n' ' , React \\n'\n",
      " 'Google Cloud Professional\\n' nan 'Front-End Web' 'React'\n",
      " 'Data Scientist , , Multi-Platform App' 'Front-End Web , React ,' 'AWS'\n",
      " 'nan' 'Full Stack Developer Backend' 'UI/UX Designer Frontend']\n",
      "--------------------------------------------------\n",
      "Cleaned unique values in 'learning_path':\n",
      "'AWS'\n",
      "'Android'\n",
      "'Back-End'\n",
      "'Backend'\n",
      "'Data Scientist'\n",
      "'Designer'\n",
      "'DevOps Engineer'\n",
      "'Developer'\n",
      "'Front-End Web'\n",
      "'Frontend'\n",
      "'Full'\n",
      "'Google Cloud Professional'\n",
      "'JavaScript'\n",
      "'Multi-Platform App'\n",
      "'React'\n",
      "'Stack'\n",
      "'UI'\n",
      "'UX'\n",
      "'iOS'\n",
      "--------------------------------------------------\n",
      "\n",
      "Original DataFrame 'learning_path' after initial string and list conversion (before explode):\n",
      "0                                      [Data Scientist]\n",
      "1     [Data Scientist, Back-End, JavaScript, DevOps ...\n",
      "2                                             [Android]\n",
      "3                                                 [iOS]\n",
      "4                                       [Front-End Web]\n",
      "5                                               [React]\n",
      "6                           [Google Cloud Professional]\n",
      "7                                                    []\n",
      "8                                       [Front-End Web]\n",
      "9                                               [React]\n",
      "10                 [Data Scientist, Multi-Platform App]\n",
      "11                               [Front-End Web, React]\n",
      "12                                                [AWS]\n",
      "13                                                   []\n",
      "14                    [Full, Stack, Developer, Backend]\n",
      "15                         [UI, UX, Designer, Frontend]\n",
      "Name: learning_path, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Import regex module\n",
    "\n",
    "# Assuming df_course is your DataFrame\n",
    "# For demonstration, let's create a sample DataFrame that mimics your data,\n",
    "# ensuring the problematic entry is present.\n",
    "data = {\n",
    "    'learning_path': [\n",
    "        'Data Scientist',\n",
    "        'Data Scientist nan Back-End JavaScript DevOps Engineer React', # This is the target for more complex splitting\n",
    "        'Android \\n',\n",
    "        ', , iOS \\n',\n",
    "        'Front-End Web \\n',\n",
    "        ' , React \\n',\n",
    "        'Google Cloud Professional\\n',\n",
    "        np.nan,\n",
    "        'Front-End Web',\n",
    "        'React',\n",
    "        'Data Scientist , , Multi-Platform App',\n",
    "        'Front-End Web , React ,',\n",
    "        'AWS',\n",
    "        'nan',\n",
    "        'Full Stack Developer Backend', # Another potential multi-concept entry\n",
    "        'UI/UX Designer Frontend'\n",
    "    ],\n",
    "    'some_other_column': range(1, 17)\n",
    "}\n",
    "df_course = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original unique values in 'learning_path':\")\n",
    "print(df_course['learning_path'].unique())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "known_paths = [\n",
    "    'Data Scientist', 'JavaScript', 'DevOps Engineer', 'React',\n",
    "    'Android', 'iOS', 'Front-End Web', 'Google Cloud Professional',\n",
    "    'Multi-Platform App', 'Back-End', 'Front-End Web'\n",
    "]\n",
    "\n",
    "known_paths_pattern = r'\\b(?:' + '|'.join(re.escape(p) for p in sorted(known_paths, key=len, reverse=True)) + r')\\b'\n",
    "\n",
    "# convert to string and handle NaN values\n",
    "df_course['learning_path'] = df_course['learning_path'].fillna('')\n",
    "df_course['learning_path'] = df_course['learning_path'].astype(str)\n",
    "df_course['learning_path'] = df_course['learning_path'].replace('nan', '', regex=False)\n",
    "\n",
    "def advanced_split_and_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = text.strip()\n",
    "    extracted_items = []\n",
    "    remaining_text = text\n",
    "\n",
    "    # extract known phrases using regex\n",
    "    found_matches = re.findall(known_paths_pattern, remaining_text, re.IGNORECASE)\n",
    "    for match in found_matches:\n",
    "        extracted_items.append(match.strip())\n",
    "        # replace the found match to avoid re-matching or interfering with further splitting\n",
    "        remaining_text = re.sub(re.escape(match), '', remaining_text, flags=re.IGNORECASE, count=1)\n",
    "\n",
    "    remaining_text = remaining_text.replace(',', ' ').replace('/', ' ') # Convert commas/slashes to spaces\n",
    "    remaining_text = re.sub(r'\\s+', ' ', remaining_text).strip() # Replace multiple spaces with single space\n",
    "\n",
    "    # Split the remaining text by spaces, if any\n",
    "    if remaining_text:\n",
    "        # Filter out short, non-meaningful words like 'nan' or single letters\n",
    "        words_from_remaining = [\n",
    "            word.strip() for word in remaining_text.split(' ')\n",
    "            if word.strip() and len(word.strip()) > 1 and word.strip().lower() != 'nan'\n",
    "        ]\n",
    "        extracted_items.extend(words_from_remaining)\n",
    "\n",
    "    # Final cleanup on all extracted items\n",
    "    cleaned_final_items = []\n",
    "    for item in extracted_items:\n",
    "        item = item.strip()\n",
    "        item = re.sub(r'[\\n\\r]+', '', item) # Remove newlines\n",
    "        item = re.sub(r'^\\s*,\\s*|\\s*,\\s*$', '', item) # Remove leading/trailing commas\n",
    "        item = re.sub(r'^,+|,+$', '', item) # Remove leading/trailing commas\n",
    "        item = item.strip(', ') # Final strip of commas and spaces\n",
    "        item = re.sub(r'\\s+', ' ', item) # Replace multiple spaces with a single space\n",
    "        if item and item.lower() != 'nan': # Final check for empty or 'nan' string\n",
    "            cleaned_final_items.append(item)\n",
    "\n",
    "    return cleaned_final_items\n",
    "\n",
    "df_course['learning_path'] = df_course['learning_path'].apply(advanced_split_and_clean)\n",
    "\n",
    "# Flatten the list of lists into a series of individual entries\n",
    "s = df_course['learning_path'].explode()\n",
    "\n",
    "# final cleaning on the exploded series \n",
    "s = s.str.strip()\n",
    "s = s.str.replace(r'\\s+', ' ', regex=True) # replace multiple spaces with a single space\n",
    "s = s.str.replace(r'[\\n\\r]+', '', regex=True) # remaining newline/carriage return chars\n",
    "s = s.str.replace(r'^\\s*,\\s*|\\s*,\\s*$', '', regex=True) # leading/trailing commas with spaces\n",
    "s = s.str.replace(r'^,+|,+$', '', regex=True) # leading/trailing commas without spaces\n",
    "s = s.str.strip(', ') # Final strip of commas and spaces\n",
    "\n",
    "# filter out empty strings and any potential NaN floats\n",
    "s = s[s != '']\n",
    "s = s.dropna() # drop any potential NaN values that might have crept in or remained as floats\n",
    "\n",
    "# get unique values\n",
    "cleaned_unique_learning_paths = s.unique()\n",
    "\n",
    "# all items are strings before sorting to prevent TypeError\n",
    "cleaned_unique_learning_paths = [str(item) for item in cleaned_unique_learning_paths]\n",
    "\n",
    "print(\"Cleaned unique values in 'learning_path':\")\n",
    "for item in sorted(cleaned_unique_learning_paths):\n",
    "    print(f\"'{item}'\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nOriginal DataFrame 'learning_path' after initial string and list conversion (before explode):\")\n",
    "print(df_course['learning_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcolab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
